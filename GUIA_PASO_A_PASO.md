# Gu√≠a Paso a Paso - Semantic Ranker para RAG

## üìã √çndice
1. [Instalaci√≥n](#instalaci√≥n)
2. [Inicio R√°pido](#inicio-r√°pido)
3. [Entrenamiento Completo](#entrenamiento-completo)
4. [Evaluaci√≥n](#evaluaci√≥n)
5. [Optimizaci√≥n](#optimizaci√≥n)
6. [Integraci√≥n con RAG](#integraci√≥n-con-rag)
7. [Puntos Importantes](#puntos-importantes)
8. [Soluci√≥n de Problemas](#soluci√≥n-de-problemas)

---

## üöÄ Instalaci√≥n

### 1. Clonar el repositorio
```bash
git clone <repository-url>
cd semantic-ranker
```

### 2. Crear entorno virtual
```bash
python -m venv venv
source venv/bin/activate  # Linux/Mac
# o
venv\Scripts\activate  # Windows
```

### 3. Instalar dependencias
```bash
pip install -r requirements.txt
```

**Nota**: La instalaci√≥n puede tardar varios minutos debido a PyTorch y Transformers.

---

## ‚ö° Inicio R√°pido

### Probar el RAG Pipeline (Sin Entrenamiento)

```bash
python examples/03_rag_pipeline.py
```

Este ejemplo usa √∫nicamente recuperaci√≥n (sin reranker) y muestra c√≥mo funciona el sistema completo.

**Lo que hace:**
- Indexa documentos de ejemplo
- Ejecuta consultas
- Muestra los documentos m√°s relevantes
- Genera contexto para LLMs

**Tiempo estimado**: 1-2 minutos

---

## üéì Entrenamiento Completo

### Paso 1: Entrenamiento B√°sico

```bash
python examples/01_basic_training.py
```

**Par√°metros importantes a modificar:**
- `max_samples`: N√∫mero de muestras (default: 1000 para demo, usa `None` para dataset completo)
- `epochs`: √âpocas de entrenamiento (default: 3, recomendado: 5-10 para producci√≥n)
- `batch_size`: Tama√±o del batch (default: 16, ajusta seg√∫n tu GPU)
- `learning_rate`: Tasa de aprendizaje (default: 2e-5)

**Salida:**
- Modelo guardado en `./models/basic_reranker/`
- Historial de entrenamiento en `training_history.json`

**Tiempo estimado**:
- Demo (1K samples): ~5-10 minutos (CPU) / ~2-3 minutos (GPU)
- Full dataset: varias horas

### Paso 2: Entrenamiento con Hard Negatives

```bash
python examples/02_hard_negative_mining.py
```

**¬øPor qu√© usar hard negatives?**
Los hard negatives son ejemplos dif√≠ciles que el modelo debe aprender a distinguir, mejorando significativamente el rendimiento.

**Mejoras esperadas:**
- +5-10% en NDCG@10
- Mejor precisi√≥n en casos ambiguos

**Tiempo estimado**:
- ~2x el tiempo del entrenamiento b√°sico (incluye miner√≠a de negativos)

### Paso 3: Workflow Completo

```bash
python examples/06_complete_workflow.py
```

Este script ejecuta todo el pipeline:
1. ‚úÖ Carga de datos
2. ‚úÖ Preprocesamiento
3. ‚úÖ Entrenamiento
4. ‚úÖ Evaluaci√≥n
5. ‚úÖ Optimizaci√≥n
6. ‚úÖ Despliegue en RAG

**Tiempo estimado**: ~30 minutos (demo) / varias horas (producci√≥n)

---

## üìä Evaluaci√≥n

### Evaluar Modelo Entrenado

```bash
python examples/04_evaluation.py
```

**M√©tricas calculadas:**
- **NDCG@k**: Calidad del ranking (0-1, mayor es mejor)
- **MRR@k**: Ranking del primer resultado relevante
- **MAP@k**: Precisi√≥n promedio
- **Hit Rate@k**: % de consultas con ‚â•1 resultado relevante

**Interpretaci√≥n de resultados:**
- NDCG@10 > 0.7: Excelente
- NDCG@10 > 0.5: Bueno
- NDCG@10 < 0.3: Necesita mejoras

**Salida:**
- Resultados guardados en `evaluation_results.json`

---

## ‚öôÔ∏è Optimizaci√≥n para Producci√≥n

```bash
python examples/05_optimization.py
```

**Optimizaciones aplicadas:**
1. **Exportaci√≥n ONNX**: Formato universal, ~10-30% m√°s r√°pido
2. **Cuantizaci√≥n INT8**: Reduce tama√±o 4x, ~2-3x m√°s r√°pido
3. **Precisi√≥n FP16**: GPU-friendly, ~2x m√°s r√°pido

**Comparaci√≥n de rendimiento:**
| Versi√≥n | Tama√±o | Latencia | Throughput |
|---------|--------|----------|------------|
| Original | 250 MB | 50 ms | 20 q/s |
| ONNX | 250 MB | 35 ms | 28 q/s |
| INT8 Quantized | 65 MB | 20 ms | 50 q/s |

**Recomendaciones:**
- **CPU**: Usa INT8 quantized
- **GPU**: Usa FP16 o ONNX
- **Edge devices**: Usa INT8 quantized + ONNX

---

## üîó Integraci√≥n con RAG

### Uso B√°sico

```python
from semantic_ranker.rag import RAGPipeline

# 1. Inicializar pipeline
pipeline = RAGPipeline(
    retriever_model="sentence-transformers/all-MiniLM-L6-v2",
    reranker_model="./models/basic_reranker/final",
    top_k_retrieval=50,  # Recuperar 50 candidatos
    top_k_rerank=5       # Reranking top-5
)

# 2. Indexar documentos
documents = ["doc1", "doc2", "doc3", ...]
pipeline.index_documents(documents)

# 3. Consultar
query = "¬øQu√© es machine learning?"
results = pipeline.retrieve_and_rerank(query)

# 4. Obtener contexto para LLM
context = pipeline.get_context_for_llm(query, top_k=3)
```

### Pipeline de Dos Etapas

```
Usuario Query
     ‚Üì
[Bi-Encoder Retrieval] ‚Üí top-50 candidatos (r√°pido, ~10ms)
     ‚Üì
[Cross-Encoder Reranking] ‚Üí top-5 mejores (preciso, ~40ms)
     ‚Üì
[LLM con Contexto] ‚Üí Respuesta final
```

**Ventajas:**
- ‚úÖ Recuperaci√≥n r√°pida con bi-encoder
- ‚úÖ Precisi√≥n alta con cross-encoder
- ‚úÖ Balance √≥ptimo velocidad/calidad

---

## ‚ö†Ô∏è Puntos Importantes

### 1. **Requisitos de Hardware**

| Tarea | CPU | GPU | RAM | Disco |
|-------|-----|-----|-----|-------|
| Inferencia | ‚úÖ | Opcional | 8 GB | 2 GB |
| Entrenamiento (demo) | ‚úÖ | Recomendado | 16 GB | 5 GB |
| Entrenamiento (full) | ‚ö†Ô∏è | ‚úÖ Requerido | 32 GB | 20 GB |

### 2. **Datos de Entrenamiento**

**Formato esperado:**
```python
{
    'query': "¬øQu√© es ML?",
    'positive': "Machine learning es...",
    'negative': "Documento irrelevante..."  # Opcional
}
```

**Tama√±o m√≠nimo recomendado:**
- **Prototipo**: 1,000 ejemplos
- **Producci√≥n**: 10,000+ ejemplos
- **√ìptimo**: 100,000+ ejemplos

### 3. **Hiperpar√°metros Clave**

```python
# Recomendaciones por escenario
ESCENARIOS = {
    'prototipo_rapido': {
        'epochs': 2,
        'batch_size': 16,
        'learning_rate': 2e-5,
        'max_samples': 1000
    },
    'produccion': {
        'epochs': 5-10,
        'batch_size': 32,
        'learning_rate': 1e-5,
        'max_samples': None
    },
    'fine_tuning': {
        'epochs': 3,
        'batch_size': 8,
        'learning_rate': 5e-6,
        'use_lora': True
    }
}
```

### 4. **Selecci√≥n de Modelo Base**

| Modelo | Tama√±o | Velocidad | Precisi√≥n | Uso Recomendado |
|--------|--------|-----------|-----------|-----------------|
| distilbert-base | 66M | ‚ö°‚ö°‚ö° | ‚≠ê‚≠ê | Desarrollo/CPU |
| bert-base | 110M | ‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê | Balanceado |
| roberta-base | 125M | ‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê | Producci√≥n |
| deberta-v3-base | 184M | ‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | M√°xima precisi√≥n |

### 5. **Monitoreo y Mantenimiento**

**M√©tricas a monitorear:**
- Latencia P50, P95, P99
- NDCG@10 en producci√≥n
- Tasa de cach√© hit
- Uso de memoria/CPU

**Cu√°ndo reentrenar:**
- ‚ö†Ô∏è NDCG@10 cae >5%
- üîÑ Nuevos datos disponibles (cada 1-3 meses)
- üÜï Cambio en distribuci√≥n de queries

### 6. **Costos de Inferencia**

**Estimaciones (1M queries/mes):**
| Setup | Costo AWS | Latencia |
|-------|-----------|----------|
| CPU (t3.medium) | ~$30/mes | 50ms |
| GPU (g4dn.xlarge) | ~$250/mes | 10ms |
| Lambda + ONNX | ~$15/mes | 100ms |

---

## üîß Soluci√≥n de Problemas

### Error: "CUDA out of memory"
**Soluci√≥n:**
```python
# Reducir batch_size
trainer.train(batch_size=8)  # en vez de 16

# Usar gradient accumulation
trainer.train(
    batch_size=8,
    gradient_accumulation_steps=2  # Efectivo: 16
)
```

### Error: "Dataset not found"
**Soluci√≥n:**
- Verificar conexi√≥n a internet
- Usar `cache_dir` personalizado
- O usar datos sint√©ticos para testing:
```python
loader = MSMARCODataLoader()
train, val, test = loader.load_and_split(max_samples=100)
```

### Rendimiento pobre en evaluaci√≥n
**Diagn√≥stico:**
1. ‚úÖ Verificar tama√±o de datos (¬ømuy pocos ejemplos?)
2. ‚úÖ Revisar calidad de negativos (¬ømuy f√°ciles?)
3. ‚úÖ Ajustar learning rate (probar 5e-6, 1e-5, 2e-5)
4. ‚úÖ Entrenar m√°s √©pocas (5-10)
5. ‚úÖ Usar hard negative mining

### Inferencia muy lenta
**Soluciones:**
1. Usar modelo cuantizado (INT8)
2. Exportar a ONNX
3. Reducir `top_k_retrieval`
4. Usar modelo m√°s peque√±o (DistilBERT)
5. Implementar batching

---

## üìö Recursos Adicionales

### Documentaci√≥n
- [Hugging Face Transformers](https://huggingface.co/docs/transformers)
- [Sentence Transformers](https://www.sbert.net/)
- [MS MARCO Dataset](https://microsoft.github.io/msmarco/)

### Papers Relevantes
- "Passage Re-ranking with BERT" (Nogueira et al., 2019)
- "ColBERT: Efficient and Effective Passage Search" (Khattab & Zaharia, 2020)
- "LoRA: Low-Rank Adaptation" (Hu et al., 2021)

### Comunidad
- GitHub Issues: Para reportar bugs
- Discussions: Para preguntas y mejores pr√°cticas

---

## üéØ Checklist de Producci√≥n

Antes de desplegar en producci√≥n, verifica:

- [ ] Modelo entrenado con datos suficientes (>10K ejemplos)
- [ ] Evaluaci√≥n NDCG@10 > 0.5
- [ ] Modelo optimizado (ONNX o cuantizado)
- [ ] Benchmarks de latencia aceptables (<100ms)
- [ ] Tests de integraci√≥n pasando
- [ ] Monitoreo configurado
- [ ] Estrategia de reentrenamiento definida
- [ ] Fallback configurado (si reranker falla)
- [ ] Documentaci√≥n actualizada
- [ ] Plan de rollback listo

---

## ü§ù Contribuciones

¬øEncontraste un bug o quieres a√±adir una feature?
1. Fork el repositorio
2. Crea una branch: `git checkout -b feature/nueva-feature`
3. Commit: `git commit -m 'Add nueva feature'`
4. Push: `git push origin feature/nueva-feature`
5. Abre un Pull Request

---

**¬øPreguntas?** Abre un issue en GitHub o consulta la documentaci√≥n completa en `/docs`.
