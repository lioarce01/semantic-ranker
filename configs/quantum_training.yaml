# Quantum Fine-Tuning Configuration
# Optimized based on cross-encoder best practices + quantum reasoning

model:
  model_name: bert-base-uncased
  max_length: 256  # Sufficient, reduces memory/time
  use_lora: true  # LoRA for efficiency
  lora_r: 8  # Standard rank (NOT 32, prevents overfitting)
  lora_alpha: 16  # 2:1 ratio with r
  lora_dropout: 0.1  # Higher dropout for regularization

training:
  epochs: 3  # CRITICAL: Quantum or not, >3 epochs = overfitting
  batch_size: 16  # Balance memory and gradient quality
  learning_rate: 0.00002  # 2e-5, proven best (conservative, good generalization)
  weight_decay: 0.01  # Standard regularization
  warmup_ratio: 0.1  # Standard 10% warmup
  max_grad_norm: 1.0  # Standard clipping
  loss_function: bce
  eval_steps: 100
  logging_steps: 50
  save_strategy: best
  gradient_accumulation_steps: 4  # Effective batch = 64
  fp16: true  # Always use mixed precision

data:
  dataset: superset_comprehensive  # Best dataset: 5503 samples, high diversity
  max_samples: null
  train_split: 0.8
  val_split: 0.1
  test_split: 0.1
  negative_sampling: hard  # Hard negatives essential for quantum coherence
  num_negatives: 2  # Balanced difficulty

quantum:
  quantum_mode: true
  resonance_threshold: 0.35  # 35% overlap for meaningful connections
  entanglement_weight: 0.2  # Proven optimal (from model 2e)
  quantum_phase: resonance  # Optimal phase for training
  knowledge_preservation_weight: 0.6  # Conservative, prevents overfitting (from model 2e)
  resonance_penalty_scale: 0.01  # Scale factor for resonance penalty (was 0.1 hardcoded)
  entanglement_loss_scale: 0.01  # Scale factor for entanglement loss (was 0.05 hardcoded)

# Quantum Reasoning (OPTIMIZED - Best of Both Worlds):
# - LR 2e-5: Proven best for generalization (Model 2e: NDCG 0.716 avg)
# - Entanglement 0.2: Proven optimal (from Model 2e)
# - Preservation 0.6: Conservative, prevents overfitting (from Model 2e)
# - Resonance scale 0.01: NEW! Breaks loss plateau (vs 0.1 hardcoded)
# - Entanglement scale 0.01: NEW! Allows BCE to converge (vs 0.05 hardcoded)
# - Result: Proven generalization (2e-5) + Lower loss floor (scales 0.01)
# - Expected: Loss ~0.30-0.40 (vs 0.75), NDCG@10 >0.72, MRR@10 >0.62

evaluation:
  metrics:
    - ndcg@10
    - ndcg@20
    - mrr@10
    - map@10
  batch_size: 32
  num_samples: null
