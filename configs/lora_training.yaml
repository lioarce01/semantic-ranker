# LoRA training configuration - memory-efficient fine-tuning
# Based on LoRA paper and 2024 best practices

model:
  model_name: bert-base-uncased
  max_length: 256  # Sufficient for most reranking tasks
  use_lora: true
  lora_r: 8  # Baseline optimal rank (balance quality/efficiency)
  lora_alpha: 16  # 2:1 ratio with r (empirically optimal)
  lora_dropout: 0.1  # QLoRA paper recommendation for smaller models

training:
  epochs: 3  # Same as standard: LoRA doesn't change overfitting dynamics
  batch_size: 32  # Larger batch possible with LoRA (fewer parameters)
  learning_rate: 0.00003  # 3e-5, slightly higher than standard (LoRA tolerance)
  weight_decay: 0.01  # Standard regularization
  warmup_ratio: 0.1  # 10% warmup
  max_grad_norm: 1.0  # Standard clipping
  loss_function: bce
  eval_steps: 100
  logging_steps: 50
  save_strategy: best
  gradient_accumulation_steps: 1  # Direct batch of 32 is efficient
  fp16: true  # Always use mixed precision

data:
  dataset: superset_comprehensive  # Best dataset: 5503 samples, high diversity
  max_samples: null
  train_split: 0.8
  val_split: 0.1
  test_split: 0.1
  negative_sampling: hard  # LoRA benefits from hard negatives
  num_negatives: 2

quantum:
  quantum_mode: false
  resonance_threshold: 0.35
  entanglement_weight: 0.3
  quantum_phase: superposition
  knowledge_preservation_weight: 0.5

evaluation:
  metrics:
    - ndcg@10
    - mrr@10
    - map@10
  batch_size: 64  # Larger batch for eval (no gradients)
  num_samples: null
