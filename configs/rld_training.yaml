# Resonant Listwise Distillation (RLD) Training Configuration
# WITH KNOWLEDGE DISTILLATION from Quantum Teacher (0.5616 NDCG@10)
# Target: NDCG@10 ~ 0.65-0.70 (+15-25% over teacher)

model:
  model_name: bert-base-uncased
  max_length: 256
  use_lora: true
  lora_r: 8
  lora_alpha: 16
  lora_dropout: 0.1

  # RLD-specific architecture
  use_set_transformer: true  # Enable listwise ranking
  set_transformer_layers: 2
  set_transformer_heads: 8
  use_mvli: false  # Token-level ColBERT matching (optional, adds +1% NDCG)

training:
  # Multi-phase training
  phase_1_epochs: 1  # Pointwise warmup (Set Transformer disabled)
  phase_2_epochs: 2  # Listwise distillation (full model)
  phase_3_epochs: 1  # Fine-tuning (lower LR)

  batch_size: 8  # Queries per batch (not pairs!)
  learning_rate: 0.00002  # 2e-5, proven optimal
  phase_3_lr: 0.000002  # 2e-6 for fine-tuning
  weight_decay: 0.01
  warmup_ratio: 0.1
  max_grad_norm: 1.0

  # Loss configuration - KNOWLEDGE DISTILLATION ENABLED
  loss_function: knowledge_distillation  # Learning from Quantum teacher!
  lambda_kd: 1.0  # KD loss dominates (learn ranking from teacher)
  lambda_bce: 0.3  # BCE smoothing (ground truth regularization)
  temperature: 1.0  # Temperature for KL divergence

  # Training settings
  eval_steps: 100
  logging_steps: 50
  save_strategy: best
  gradient_accumulation_steps: 4  # Effective batch = 32 queries
  fp16: true

data:
  dataset: msmarco_small  # Using small for faster training with KD
  max_queries: null  # Use all queries
  max_docs_per_query: 50  # Sample if more
  min_docs_per_query: 2  # Filter if fewer

  # Optional: Pre-computed embeddings (PECC)
  use_pecc: false  # Enable for 3x speedup (not implemented yet)
  pecc_model: all-mpnet-base-v2

  # Knowledge distillation - TEACHER ENABLED!
  teacher_model_path: models/quantum_base_resonance_5k_2e_optimized  # Quantum teacher (0.5616 NDCG@10)
  teacher_scores_path: cache/teacher_scores/msmarco_small_teacher.json  # Pre-computed scores

  # Data splits (by queries, not samples)
  train_split: 0.8
  val_split: 0.1
  test_split: 0.1

set_transformer:
  d_model: 768  # Must match BERT hidden size
  num_heads: 8
  num_layers: 2
  d_ff: 2048
  dropout: 0.1
  use_positional_encoding: true  # Rank-aware positions

distillation:
  use_teacher: true  # ENABLED - Learning from Quantum teacher
  temperature: 1.0
  adaptive_temperature: false  # Use fixed temperature (can enable for auto-tuning)

evaluation:
  metrics:
    - ndcg@10
    - ndcg@20
    - mrr@10
    - map@10
  batch_size: 16  # Queries for evaluation
  num_samples: null

# Expected Performance WITH TEACHER (Knowledge Distillation):
# - Teacher (Quantum 5k): NDCG@10 = 0.5616
# - RLD Target: NDCG@10 = 0.65-0.70 (+15-25% over teacher)
# - Training time: ~6-8 hours on CPU (4 epochs, smaller dataset)
#
# What RLD Learns from Teacher:
# 1. Set Transformer: +10-15% NDCG (listwise context + KD)
# 2. Knowledge Distillation: +20-30% over no-teacher baseline
# 3. Ranking nuances: Not just relevant (1) vs irrelevant (0), but degrees (0.87 vs 0.23)
# 4. Teacher transfers its knowledge: Student learns what makes docs "very relevant" vs "somewhat relevant"
#
# Comparison:
# - RLD without teacher: NDCG@10 ~ 0.45-0.50 (listwise loss only)
# - RLD with teacher: NDCG@10 ~ 0.65-0.70 (+40% improvement!)
# - Goal: Match or exceed teacher while being 3x faster inference
