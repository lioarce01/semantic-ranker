# Query Graph Neural Reranking Configuration
# Novel research approach: First GNN-based reranker using query graphs
# Config based on quantum_base_resonance_5k_2e_optimized (NDCG 0.78 avg)

model:
  model_name: bert-base-uncased
  max_length: 256
  use_lora: true
  lora_r: 8
  lora_alpha: 16
  lora_dropout: 0.1

training:
  epochs: 3
  batch_size: 16
  learning_rate: 2.0e-05  # 2e-5, proven optimal from quantum model
  weight_decay: 0.01
  warmup_ratio: 0.1
  max_grad_norm: 1.0
  loss_function: bce
  eval_steps: 100
  logging_steps: 50
  save_strategy: best
  gradient_accumulation_steps: 4  # Effective batch = 64
  fp16: true

data:
  dataset: superset_comprehensive  # Same as quantum_base_resonance_5k_2e_optimized
  max_samples: null  # All 5503 samples
  train_split: 0.8
  val_split: 0.1
  test_split: 0.1
  negative_sampling: hard  # Hard negatives crucial for performance
  num_negatives: 2

gnn:
  gnn_mode: true
  embedding_model: all-mpnet-base-v2  # 768-dim semantic embeddings
  similarity_threshold: 0.65  # Slightly lower for more edges (was 0.7)
  max_neighbors: 10  # Max neighbors per query node
  gnn_hidden_dim: 256  # GNN first layer
  gnn_output_dim: 128  # GNN second layer (final query embedding)
  gnn_dropout: 0.1  # Dropout in GNN layers
  lambda_contrastive: 0.1  # Weight for contrastive loss in query space
  lambda_rank: 0.05  # Weight for GNN ranking loss
  temperature: 0.07  # Temperature for InfoNCE contrastive loss

quantum:
  quantum_mode: false  # Disable quantum for QG-Rerank

evaluation:
  metrics:
    - ndcg@10
    - ndcg@20
    - mrr@10
    - map@10
  batch_size: 32
  num_samples: null
