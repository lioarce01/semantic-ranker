# ğŸ“š Glosario de Machine Learning para EvaluaciÃ³n de Modelos

## ğŸ” **TÃ©rminos BÃ¡sicos de Entrenamiento**

### **Loss (PÃ©rdida)**
Medida de error del modelo. `Loss = 0.0` (perfecto), `0.1-0.5` (bueno), `>1.0` (aprendiendo). **Loss bajando** = âœ… aprendizaje.

### **Epoch (Ã‰poca)**
Una pasada completa por todo el dataset. **MÃ¡s epochs** = mÃ¡s aprendizaje, pero riesgo de **overfitting**.

### **Batch Size**
Ejemplos procesados antes de actualizar parÃ¡metros. `batch_size=8-32` (estable), `128+` (rÃ¡pido pero inestable).

### **Learning Rate**
TamaÃ±o del "paso" de aprendizaje. `1e-5` (estable), `2e-5` (tÃ­pico), `1e-3` (muy grande).

### **Overfitting**
Modelo memoriza datos de entrenamiento pero falla en nuevos. **SÃ­ntomas**: Train loss bajo, validation alto.

### **Underfitting**
Modelo no aprende suficiente. **SÃ­ntomas**: Train/validation loss altos similares.

## ğŸ“Š **MÃ©tricas de EvaluaciÃ³n**

### **Train vs Validation Loss**
- **Train Loss**: Rendimiento en datos de entrenamiento
- **Validation Loss**: GeneralizaciÃ³n a datos nuevos
- **Ideal**: Ambos bajos y cercanos

### **Convergencia**
Modelo deja de mejorar. **Indicadores**: Loss estable 2-3 epochs, train â‰ˆ validation.

## ğŸ—ï¸ **Arquitectura del Modelo**

### **Cross-Encoder**
Procesa query + documento juntos para ranking. âœ… Preciso, âŒ lento.

### **LoRA (Low-Rank Adaptation)**
Fine-tuning eficiente entrenando pocos parÃ¡metros. âœ… 10x menos memoria, âœ… mismo rendimiento.

## ğŸ“ˆ **MÃ©tricas de Rendimiento**

### **NDCG@k**
Calidad del ranking top-k. `â‰¥0.85` (excelente), `â‰¥0.70` (bueno), `<0.60` (necesita mejoras).

### **MRR@k**
PosiciÃ³n del primer resultado relevante. `â‰¥0.85` (excelente), `â‰¥0.70` (bueno).

### **Latency**
Tiempo de procesamiento. `<50ms` (excelente), `50-200ms` (aceptable), `>500ms` (lento).

## ğŸ”§ **TÃ©cnicas de Entrenamiento**

### **Retrain**
Continuar entrenando modelo existente con mÃ¡s datos. âœ… Menos riesgoso que desde cero.

### **Hard Negative Mining**
Entrenar con ejemplos difÃ­ciles que el modelo confunde. +0.05-0.10 NDCG.

### **Quantum Fine Tuning** ğŸ§¬
Framework que usa similitud lÃ©xica (Jaccard) y grafos de queries para ajuste fino.

#### **Conceptos BÃ¡sicos**
- **Quantum Resonance**: Similitud por word overlap entre query-documento
- **Entanglement Graph**: Grafo de queries relacionadas por Jaccard similarity
- **Resonance Frequency**: Overlap ratio = |Q âˆ© D| / |Q âˆª D|

#### **ParÃ¡metros Clave**
- `resonance_threshold`: 0.35 - Umbral de similitud para crear edges
- `entanglement_weight`: 0.2 - Peso de pÃ©rdida de entanglement
- `knowledge_preservation_weight`: 0.6 - PreservaciÃ³n de conocimiento previo
- `resonance_penalty_scale`: 0.01 - Escala de penalizaciÃ³n
- `entanglement_loss_scale`: 0.01 - Escala de pÃ©rdida de entanglement

#### **Loss Function**
```
L_total = L_BCE + (resonance_penalty Ã— 0.01) + (entanglement_loss Ã— 0.01 Ã— 0.2)
```

#### **Resultados**
- **Best Model**: quantum_base_resonance_5k_2e_optimized
- **NDCG@10**: 0.7847 (avg) - superset, qa_mixed, natural_questions
- **Loss**: 0.11 (vs 0.73-0.75 en modelos previos)

### **Query Graph Neural Reranking (QG-Rerank)** ğŸ¯
**Novel research approach**: Primer reranker con GNN sobre grafos de queries (no documentos).

#### **Conceptos Fundamentales**
- **Query Graph**: Grafo semÃ¡ntico donde nodos = queries, edges = similitud semÃ¡ntica
- **Query Clustering Hypothesis**: Si doc D es relevante para Q1, y Q2 es similar a Q1, entonces D es relevante para Q2
- **Cross-Query Learning**: Transferencia de conocimiento entre queries similares vÃ­a message passing
- **Semantic Embeddings**: SentenceTransformer (all-mpnet-base-v2) para similitud profunda, no lÃ©xica

#### **Arquitectura**
```
Query â†’ SentenceTransformer (768-dim) â†’ Query Graph â†’ GNN (2-layer GCN)
                                                         â†“
                                                    128-dim refined embeddings
                                                         â†“
Query-Doc â†’ Cross-Encoder (BERT) â†’ Hidden States â†’ Attention Layer â†’ Prediction
                                        â†‘_______________|
```

#### **Graph Neural Network**
- **Layer 1**: GCN (768 â†’ 256) + ReLU + LayerNorm + Dropout
- **Layer 2**: GCN (256 â†’ 128) + LayerNorm
- **Message Passing**: AgregaciÃ³n de informaciÃ³n de queries vecinas ponderada por similitud

#### **Multi-Task Loss**
```
L_total = L_BCE + Î»_contrastive Ã— L_contrastive + Î»_rank Ã— L_rank
```
- **L_BCE**: Binary cross-entropy (relevancia punto a punto)
- **L_contrastive**: InfoNCE en espacio de queries (queries con docs relevantes compartidos deben ser similares)
- **L_rank**: Ranking loss con embeddings GNN (queries con mÃ¡s docs relevantes â†’ mayor norma)

#### **ParÃ¡metros Clave**
- `similarity_threshold`: 0.7 - Similitud mÃ­nima coseno para crear edge
- `max_neighbors`: 10 - MÃ¡ximo vecinos por nodo de query
- `gnn_hidden_dim`: 256 - DimensiÃ³n capa oculta GNN
- `gnn_output_dim`: 128 - DimensiÃ³n salida GNN (query embedding final)
- `lambda_contrastive`: 0.1 - Peso de pÃ©rdida contrastiva
- `lambda_rank`: 0.05 - Peso de pÃ©rdida de ranking
- `temperature`: 0.07 - Temperatura para InfoNCE

#### **Ventajas vs Quantum**
- âœ… **Semantic Understanding**: Embeddings densos (768-dim) vs Jaccard lÃ©xico
- âœ… **GNN Message Passing**: PropagaciÃ³n de informaciÃ³n vs penalties estÃ¡ticos
- âœ… **Contrastive Learning**: Aprendizaje en espacio latente de queries
- âœ… **Cross-Query Transfer**: GeneralizaciÃ³n a queries no vistas pero similares

#### **ComparaciÃ³n con SOTA**
| Approach | Graph Type | Similarity | Message Passing |
|----------|-----------|-----------|-----------------|
| G-RAG | Document graph | Doc embeddings | GNN over docs |
| GNRR | Corpus graph | BM25 retrieval | GNN over corpus |
| Quantum FT | Query graph | Jaccard | Penalties only |
| **QG-Rerank** | **Query graph** | **Semantic embeddings** | **GNN over queries** |

#### **Research Novelty**
- ğŸ”¬ **Primera aplicaciÃ³n de GNN sobre grafos de queries** (no docs)
- ğŸ”¬ Query Clustering Hypothesis (extensiÃ³n del doc clustering hypothesis)
- ğŸ”¬ Framework de transferencia cross-query
- ğŸ”¬ Multi-task learning: relevance + contrastive + ranking

#### **Expected Benefits**
- **Zero-shot**: Mejor desempeÃ±o en queries fuera del dominio
- **Sparse queries**: Transferencia desde queries densas
- **Domain shift**: Captura patrones a nivel de query, no solo documento

#### **ImplementaciÃ³n**
```bash
# Train QG-Rerank
python -m cli.qg_train --config configs/qg_rerank.yaml --model-name qg_rerank_v1

# Evaluate
python -m cli.eval --model-path models/qg_rerank_v1/best --dataset superset_comprehensive
```

## ğŸ“Š **InterpretaciÃ³n de Logs**

### **Training Progress**
```
Epoch 1/5
INFO: Loss: 1.2485 â†’ Average Loss: 1.1385
```
- **Loss bajando**: âœ… Aprendizaje progresando
- **Average Loss**: MÃ©trica principal por epoch

### **Modelos State-of-the-Art**
- **BGE-Reranker-v2.0**: NDCG@10 = 0.866 (lÃ­der actual)
- **FlashRank**: NDCG@10 = 0.842 (muy competitivo)
- **MonoT5**: NDCG@10 = 0.814 (arquitectura probada)

## ğŸ¯ **Estado Actual del Proyecto**

### **Modelo Quantum (quantum_base_resonance_5k_2e_optimized)**
**Ãšltima evaluaciÃ³n - Resultados excelentes:**

| Dataset | NDCG@10 | MRR@10 | MAP@10 |
|---------|---------|---------|---------|
| qa_mixed_giant | 0.8003 | 0.7308 | 0.7308 |
| natural_questions | 0.8326 | 0.7767 | 0.7767 |
| superset_comprehensive | 0.7213 | 0.6185 | 0.6283 |
| **AVERAGE** | **0.7847** | **0.7087** | - |

**Estado**: âœ… **Excelente desempeÃ±o** - competitivo con modelos SOTA comerciales
- **Fortaleza destacada**: natural_questions (NDCG 0.83) - +28.2% vs modelo anterior
- **Loss final**: 0.11 (vs 0.73-0.75 en modelos previos)
- **Config Ã³ptima**: LR 2e-5, entanglement 0.2, preservation 0.6, scales 0.01

**EvaluaciÃ³n en progreso**: BEIR benchmark (zero-shot performance en dominios no vistos)

### **PrÃ³ximos Pasos**
1. âœ… Completar evaluaciÃ³n BEIR para medir generalizaciÃ³n
2. ğŸ”¬ Entrenar QG-Rerank y comparar vs Quantum
3. ğŸ“Š Benchmark comparison con modelos SOTA (BGE, FlashRank)
4. ğŸ“ Documentar hallazgos para publicaciÃ³n acadÃ©mica